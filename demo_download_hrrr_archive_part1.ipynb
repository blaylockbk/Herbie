{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brian Blaylock**  \n",
    "**June 24, 2020**  \n",
    "\n",
    "üåê HRRR Archive Website: http://hrrr.chpc.utah.edu/  \n",
    "üöë Support: atmos-mesowest@lists.utah.edu  \n",
    "üìß Brian Blaylock: blaylockbk@gmail.com  \n",
    "‚úí Citation this details:\n",
    "> Blaylock B., J. Horel and S. Liston, 2017: Cloud Archiving and Data Mining of High Resolution Rapid Refresh Model Output. Computers and Geosciences. 109, 43-50. https://doi.org/10.1016/j.cageo.2017.08.005\n",
    "\n",
    "---\n",
    "\n",
    "# üèó HRRR Download Demo: Part 1\n",
    "## How to download a bunch of HRRR GRIB2 files\n",
    "\n",
    "- [Part 1: How to download a bunch of HRRR grib2 files (full file)](https://github.com/blaylockbk/HRRR_archive_download/blob/master/demo_download_hrrr_archive_part1.ipynb)\n",
    "- [Part 2: How to download a subset of variables from a HRRR file](https://github.com/blaylockbk/HRRR_archive_download/blob/master/demo_download_hrrr_archive_part2.ipynb)\n",
    "- [Part 3: A function that can download many full files, or subset of files](https://github.com/blaylockbk/HRRR_archive_download/blob/master/demo_download_hrrr_archive_part3.ipynb)\n",
    "- [Part 4: Opening GRIB2 files in Python with `xarray` and `cfgrib`](https://github.com/blaylockbk/HRRR_archive_download/blob/master/demo_download_hrrr_archive_part4.ipynb)\n",
    "\n",
    "---\n",
    "The GRIB2 file format is a common format used to distrubute many types of weather model data, including the Hight Resolution Rapid Refresh model.\n",
    "\n",
    "HRRR GRIB2 files can be downloaded from the [University of Utah's HRRR archive](http://hrrr.chpc.utah.edu/) on the CHPC Pando archive system. You may also get HRRR GRIB2 files from the [NOAA Operational Model Archive and Distribution System (NOMADS)](https://nomads.ncep.noaa.gov/), but only for the today's and yesterday's runs.\n",
    "\n",
    "This notebook shows how you can download many HRRR files for a list of datetimes and forecast hours.\n",
    "\n",
    "Let's start by importing some modules we will use..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import urllib.request  # Used to download the file\n",
    "import requests        # Used to check if a URL exists\n",
    "import warnings\n",
    "\n",
    "import pandas as pd    # Just used for the date_range function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading a file from the internet with Python is pretty straight forward. All you need is the file URL. For example, for a single HRRR file downloaded from the Pando archive, we can do this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_this = 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200624/hrrr.t01z.wrfsfcf17.grib2'\n",
    "save_as = 'my_file.grib2'\n",
    "a, b= urllib.request.urlretrieve(download_this, save_as)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some Jupyter Cell Magic, and if we have wgrib2 installed, we can print some grib messages for the file we just downloaded to show that we did indeed download a GRIB2 file.\n",
    "\n",
    "> Note: `wgrib2` is not available on Windows (without suffering a major headache), so if you are on a Windows machine, don't sweat this step. There are other ways to look at GRIB2 data on your computer with pygrib or cfgrib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:7852240:d=2020062401:TMP:500 mb:17 hour fcst:\n",
      "19:11260176:d=2020062401:TMP:700 mb:17 hour fcst:\n",
      "24:14834021:d=2020062401:TMP:850 mb:17 hour fcst:\n",
      "28:17825413:d=2020062401:TMP:925 mb:17 hour fcst:\n",
      "32:20789480:d=2020062401:TMP:1000 mb:17 hour fcst:\n",
      "59:39358136:d=2020062401:TMP:surface:17 hour fcst:\n",
      "66:42415868:d=2020062401:TMP:2 m above ground:17 hour fcst:\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wgrib2 my_file.grib2 -match \":TMP:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a script to download many HRRR files is just a matter of looping and modifying the URL of the target file we download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This next cell is rather lengthy, but it includes the `download_HRRR` function, which we will use to download HRRR files from the University of Utah archive or from NOMADS. Read the document string to understand all the options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reporthook(a, b, c):\n",
    "    \"\"\"\n",
    "    Report download progress in megabytes (prints progress to screen).\n",
    "    \n",
    "    This is used when we download a file with ``urllib.request.urlretrieve``.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : Chunk number\n",
    "    b : Maximum chunk size\n",
    "    c : Total size of the download\n",
    "    \"\"\"\n",
    "    chunk_progress = a * b / c * 100\n",
    "    total_size_MB =  c / 1000000.\n",
    "    print(f\"\\r Download Progress: {chunk_progress:.2f}% of {total_size_MB:.1f} MB\\r\", end='')\n",
    "\n",
    "def download_HRRR(DATES, fxx=range(0, 1), model='hrrr', field='sfc', \n",
    "                  SOURCE='pando', SAVEDIR='./', dryrun=False):\n",
    "    \"\"\"\n",
    "    Downloads full HRRR grib2 files for a list of dates and forecasts.\n",
    "    \n",
    "    Files are downloaded from the University of Utah HRRR archive (Pando) \n",
    "    or NOAA Operational Model Archive and Distribution System (NOMADS).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    DATES : datetime or list of datetimes\n",
    "        A datetime or list of datetimes that represent the model \n",
    "        initialization time for which you want to download.\n",
    "    fxx : int or list of ints\n",
    "        Forecast lead time or list of forecast lead times to download.\n",
    "        Default only grabs analysis hour (f00), but you might want all\n",
    "        the forecasts hours, in that case, you could set ``fxx=range(0,19)``.\n",
    "    model : {'hrrr', 'hrrrak', 'hrrrX'}\n",
    "        The model type you want to download.\n",
    "        - 'hrrr' HRRR Contiguous United States (operational)\n",
    "        - 'hrrrak' HRRR Alaska. You can also use 'alaska' as an alias.\n",
    "        - 'hrrrX' HRRR *experimental*\n",
    "    field : {'prs', 'sfc', 'nat', 'subh'}\n",
    "        Variable fields you wish to download. \n",
    "        - 'sfc' surface fields\n",
    "        - 'prs' pressure fields\n",
    "        - 'nat' native fields      ('nat' files are not available on Pando)\n",
    "        - 'subh' subhourly fields  ('subh' files are not available on Pando)\n",
    "    SOURCE : {'pando', 'nomads'}\n",
    "        Specify the source from which to download the HRRR files.\n",
    "        - 'pando' downloads HRRR files from University of Utah archive:\n",
    "        http://hrrr.chpc.utah.edu/        \n",
    "        - 'nomads' downloads HRRR files from NCEP NOMADS server:\n",
    "        https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/\n",
    "    SAVEDIR : str\n",
    "        Directory path to save the downloaded HRRR files.\n",
    "    dryrun : bool\n",
    "        If True, instead of downloading the files, it will print out the\n",
    "        files that could be downloaded. This is set to False by default.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Downloads the HRRR files, with filename prepended with the run date\n",
    "    (i.e. `20170101_hrrr.t00z.wrfsfcf00.grib2`)\n",
    "    \"\"\"\n",
    "    \n",
    "    #**************************************************************************\n",
    "    ## Check function input\n",
    "    #**************************************************************************\n",
    "    \n",
    "    # Ping Pando first. This *might* prevent a \"bad handshake\" error.\n",
    "    if SOURCE == 'pando':\n",
    "        try:\n",
    "            requests.head('https://pando-rgw01.chpc.utah.edu/')\n",
    "        except:\n",
    "            print('bad handshake...am I able to on?')\n",
    "            pass\n",
    "    \n",
    "    # Force the `SOURCE` and `field` input string to be lower case.\n",
    "    SOURCE = SOURCE.lower()\n",
    "    field = field.lower()\n",
    "\n",
    "    # `DATES` and `fxx` should be a list-like object, but if it doesn't have\n",
    "    # length, (like if the user requests a single date or forecast hour),\n",
    "    # then turn it item into a list-like object.\n",
    "    if not hasattr(DATES, '__len__'): DATES = np.array([DATES])\n",
    "    if not hasattr(fxx, '__len__'): fxx = [fxx]\n",
    "    \n",
    "    # HRRR data on NOMADS is only available for today's and yesterday's runs.\n",
    "    # If any of the DATES are older than yesterday, raise a warning and\n",
    "    # change SOURCE to pando.\n",
    "    if SOURCE == 'nomads':\n",
    "        yesterday = datetime.utcnow() - timedelta(days=1)\n",
    "        yesterday = datetime(yesterday.year, yesterday.month, yesterday.day)\n",
    "        if any(DATES < yesterday):\n",
    "            warnings.warn(\"Changed the SOURCE to 'pando' because one or more of the requested DATES are for more than two days ago.\")\n",
    "            SOURCE = 'pando'\n",
    "    \n",
    "    # The user may set `model='alaska'` as an alias for 'hrrrak'.\n",
    "    if model.lower() == 'alaska': model = 'hrrrak'\n",
    "      \n",
    "    _SOURCE = {'pando', 'nomads'}\n",
    "    assert SOURCE in _SOURCE, f'`SOURCE` must be one of {_SOURCE}'\n",
    "    \n",
    "    # The model type and field depends on the SOURCE the files are downloaded.\n",
    "    if SOURCE == 'pando':\n",
    "        _models = {'hrrr', 'hrrrak', 'hrrrX'}\n",
    "        _fields = {'sfc', 'prs'}\n",
    "    elif SOURCE == 'nomads':\n",
    "        _models = {'hrrr', 'hrrrak'}\n",
    "        _fields = {'sfc', 'prs', 'nat', 'subh'}\n",
    "        \n",
    "    assert model in _models, f'`model` should be set to one of {_models} for `SOURCE={SOURCE}`'\n",
    "    assert field in _fields, f'`field` should be set to one of {_fields} for `SOURCE={SOURCE}`'\n",
    "    \n",
    "    # Make SAVEDIR if path doesn't exist\n",
    "    if not os.path.exists(SAVEDIR):\n",
    "        os.makedirs(SAVEDIR)\n",
    "        print(f'Created directory: {SAVEDIR}')\n",
    "\n",
    "    #**************************************************************************\n",
    "    # Build the URL path for every file we want\n",
    "    #**************************************************************************\n",
    "    # An example URL for a file from Pando is \n",
    "    # https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200624/hrrr.t01z.wrfsfcf17.grib2\n",
    "    # \n",
    "    # An example URL for a file from NOMADS is\n",
    "    # https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t00z.wrfsfcf09.grib2\n",
    "    \n",
    "    # `base_url`    : The first part of the URL path\n",
    "    # `file_list`   : A list of full URL paths to each file (one for each \n",
    "    #                 forecast hour requested)\n",
    "    # `file_rename` : A list of names the files will be renamed. It prepends\n",
    "    #                 the original file name with the run date YYYYMMDD.\n",
    "        \n",
    "    if SOURCE == 'pando':\n",
    "        base = f'https://pando-rgw01.chpc.utah.edu/{model}/{field}'\n",
    "        URL_list = [f'{base}/{DATE:%Y%m%d}/{model}.t{DATE:%H}z.wrf{field}f{f:02d}.grib2' for DATE in DATES for f in fxx]\n",
    "    \n",
    "    elif SOURCE == 'nomads':\n",
    "        base = 'https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod'\n",
    "        if model == 'hrrr':\n",
    "            URL_list = [f'{base}/hrrr.{DATE:%Y%m%d}/conus/hrrr.t{DATE:%H}z.wrf{field}f{f:02d}.grib2' for DATE in DATES for f in fxx]\n",
    "        elif model == 'hrrrak':\n",
    "            URL_list = [f'{base}/hrrr.{DATE:%Y%m%d}/alaska/hrrr.t{DATE:%H}z.wrf{field}f{f:02d}.ak.grib2' for DATE in DATES for f in fxx]\n",
    "    \n",
    "    #**************************************************************************\n",
    "    # Ok, so we have a URL and filename for each requested forecast hour.\n",
    "    # Now we need to check if each of those files exist, and if it does,\n",
    "    # we will download that file to the SAVEDIR location.\n",
    "    \n",
    "    for file_URL in URL_list:\n",
    "        # We want to prepend the filename with the run date, YYYYMMDD\n",
    "        if SOURCE == 'pando':\n",
    "            rename = '_'.join(file_URL.split('/')[-2:])\n",
    "        elif SOURCE == 'nomads':\n",
    "            rename = file_URL.split('/')[-3][5:] + '_' + file_URL.split('/')[-1]\n",
    "        \n",
    "        # Check if the URL returns a status code of 200 (meaning the URL is ok)\n",
    "        # Also check that the Content-Length is >1000000 bytes (if it's smaller,\n",
    "        # the file on the server might be incomplete)\n",
    "        head = requests.head(file_URL)\n",
    "        \n",
    "        check_exists = head.ok\n",
    "        check_content = int(head.raw.info()['Content-Length']) > 1000000\n",
    "        \n",
    "        if check_exists and check_content:\n",
    "            # Download the file\n",
    "            if dryrun:\n",
    "                print(f'üåµ Dry Run Success! Would have downloaded {file_URL} as {SAVEDIR+rename}')\n",
    "            else:\n",
    "                urllib.request.urlretrieve(file_URL, SAVEDIR+rename, reporthook)\n",
    "                print(f'‚úÖ Success! Downloaded {file_URL} as {SAVEDIR+rename}')\n",
    "        else:\n",
    "            # The URL request is bad. If status code == 404, the URL does not exist.\n",
    "            print()\n",
    "            print(f'‚ùå WARNING: Status code {head.status_code}: {head.reason}. Content-Length: {int(head.raw.info()[\"Content-Length\"]):,} bytes')\n",
    "            print(f'‚ùå Could not download {head.url}')\n",
    "    \n",
    "    print(\"\\nFinished üç¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples...\n",
    "Ok, now that you have the `download_HRRR` function, we need to tell it what we want to download.\n",
    "\n",
    "Let's start with a range of dates. We imported the Pandas module just because I really like the `date_range` function.\n",
    "\n",
    "> Note: These dates refer to the model's *initialization* time.\n",
    "\n",
    "We use python's standard `datetime` module to define a start date and end date. Then we use pandas `date_range` function to create a list of dates. We set the freq to be `freq='1H'` becuase we know the HRRR model runs every hour. If you don't want every hour of HRRR data, you could change the frequency (e.g., '3H' would create a list of dates for every 3 hours between the start and end time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2020-04-24 00:00:00', '2020-04-24 01:00:00',\n",
       "               '2020-04-24 02:00:00', '2020-04-24 03:00:00'],\n",
       "              dtype='datetime64[ns]', freq='H')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the start and end date for the HRRR files we want to download\n",
    "sDATE = datetime(2020, 4, 24)\n",
    "eDATE = datetime(2020, 4, 24, 3)\n",
    "\n",
    "# Create a list of datetimes we want to download with Pandas `date_range` function.\n",
    "# The HRRR model is run every hour, so make a list of every hour\n",
    "DATES = pd.date_range(sDATE, eDATE, freq='1H')\n",
    "DATES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other thing you might need to specify is which forecast hours to download. By default, the function will only download the analyis (F00, zero-hour forecast).\n",
    "I like to use `fxx` as my variable for a list of forecast hours, becuase to me it looks like **F00**, **F02**, **F12**, etc.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "|Code|Output\n",
    "|---|---\n",
    "|`fxx = range(0, 1)`| [0] *this is the function's default*\n",
    "|`fxx = range(0, 19)`| [0, 1, 2, 3, ..., 18]\n",
    "|`fxx = range(0, 19, 3)`| [0, 3, 6, 9, 12, 15, 18]\n",
    "|`fxx = [2, 6, 7, 12]`| [2, 6, 7, 12] *of course, you can make your own list*\n",
    "\n",
    "For this example, lets just download **F00** and **F03**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fxx = range(0, 4, 3)\n",
    "list(fxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call the `download_HRRR` function with our specified DATES and forecasts hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success! Downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t00z.wrfsfcf00.grib2 as ./20200424_hrrr.t00z.wrfsfcf00.grib2\n",
      "‚úÖ Success! Downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t00z.wrfsfcf03.grib2 as ./20200424_hrrr.t00z.wrfsfcf03.grib2\n",
      "‚úÖ Success! Downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t01z.wrfsfcf00.grib2 as ./20200424_hrrr.t01z.wrfsfcf00.grib2\n",
      "‚úÖ Success! Downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t01z.wrfsfcf03.grib2 as ./20200424_hrrr.t01z.wrfsfcf03.grib2\n",
      "‚úÖ Success! Downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t02z.wrfsfcf00.grib2 as ./20200424_hrrr.t02z.wrfsfcf00.grib2\n",
      "‚úÖ Success! Downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t02z.wrfsfcf03.grib2 as ./20200424_hrrr.t02z.wrfsfcf03.grib2\n",
      "‚úÖ Success! Downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t03z.wrfsfcf00.grib2 as ./20200424_hrrr.t03z.wrfsfcf00.grib2\n",
      "‚úÖ Success! Downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t03z.wrfsfcf03.grib2 as ./20200424_hrrr.t03z.wrfsfcf03.grib2\n",
      "\n",
      "Finished üç¶\n"
     ]
    }
   ],
   "source": [
    "download_HRRR(DATES, fxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That just downloaded the HRRR files into the current working directory.\n",
    "\n",
    "We can specify the directory we want to save the files to with the `SAVEDIR` argument. If the directory path doesn't exist, then it will be created. And one more thing...we can do a \"dry run\" of the download, meaning we go through the motions of the function, but skip the actual download. This will show how the download will work. Let's turn on the \"dry run\" for this next test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: ./putInThisDir/\n",
      "üåµ Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t00z.wrfsfcf00.grib2 as ./putInThisDir/20200424_hrrr.t00z.wrfsfcf00.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t00z.wrfsfcf03.grib2 as ./putInThisDir/20200424_hrrr.t00z.wrfsfcf03.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t01z.wrfsfcf00.grib2 as ./putInThisDir/20200424_hrrr.t01z.wrfsfcf00.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t01z.wrfsfcf03.grib2 as ./putInThisDir/20200424_hrrr.t01z.wrfsfcf03.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t02z.wrfsfcf00.grib2 as ./putInThisDir/20200424_hrrr.t02z.wrfsfcf00.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t02z.wrfsfcf03.grib2 as ./putInThisDir/20200424_hrrr.t02z.wrfsfcf03.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t03z.wrfsfcf00.grib2 as ./putInThisDir/20200424_hrrr.t03z.wrfsfcf00.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t03z.wrfsfcf03.grib2 as ./putInThisDir/20200424_hrrr.t03z.wrfsfcf03.grib2\n",
      "\n",
      "Finished üç¶\n"
     ]
    }
   ],
   "source": [
    "download_HRRR(DATES, fxx, SAVEDIR='./putInThisDir/', dryrun=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More options\n",
    "\n",
    "The useage above is what most people want from the HRRR archive, but there are three other options we can change.\n",
    "\n",
    "**`model=`**\n",
    "- `'hrrr'`Download the operational HRRR for the contiguous 48 states. *This is the default.*\n",
    "- `'hrrrak'` or `'alaska'` Download the operational HRRR-Alaska domain.\n",
    "- `'hrrrX'` Download the experimental HRRR. **Not available from NOMADS but some analyses are on Pando.**\n",
    "\n",
    "**`field=`**\n",
    "- `'sfc'` Download the surface fields. *This is the default.*\n",
    "- `'prs'` Download the pressure fields\n",
    "- `'nat'` Download the native fields. **Not available on Pando**\n",
    "- `'subh'` Download the subhourly fields. **Not available on Pando**\n",
    "\n",
    "**`SOURCE=`**\n",
    "- `'pando'` Download files from the University of Utah's Pando archive system. *This is the default.*\n",
    "- `'nomads'` Download files from NOMADS server. Files are only available for today and yesterday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it look like when we download Alaska grids from NOMADS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/home/blaylock/anaconda3/envs/basic/lib/python3.7/site-packages/ipykernel_launcher.py:92: UserWarning: Changed the SOURCE to 'pando' because one or more of the requested DATES are for more than two days ago.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåµ Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrrak/sfc/20200424/hrrrak.t00z.wrfsfcf00.grib2 as ./20200424_hrrrak.t00z.wrfsfcf00.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrrak/sfc/20200424/hrrrak.t00z.wrfsfcf03.grib2 as ./20200424_hrrrak.t00z.wrfsfcf03.grib2\n",
      "\n",
      "‚ùå WARNING: Status code 404: Not Found. Content-Length: 219 bytes\n",
      "‚ùå Could not download https://pando-rgw01.chpc.utah.edu/hrrrak/sfc/20200424/hrrrak.t01z.wrfsfcf00.grib2\n",
      "\n",
      "‚ùå WARNING: Status code 404: Not Found. Content-Length: 219 bytes\n",
      "‚ùå Could not download https://pando-rgw01.chpc.utah.edu/hrrrak/sfc/20200424/hrrrak.t01z.wrfsfcf03.grib2\n",
      "\n",
      "‚ùå WARNING: Status code 404: Not Found. Content-Length: 219 bytes\n",
      "‚ùå Could not download https://pando-rgw01.chpc.utah.edu/hrrrak/sfc/20200424/hrrrak.t02z.wrfsfcf00.grib2\n",
      "\n",
      "‚ùå WARNING: Status code 404: Not Found. Content-Length: 219 bytes\n",
      "‚ùå Could not download https://pando-rgw01.chpc.utah.edu/hrrrak/sfc/20200424/hrrrak.t02z.wrfsfcf03.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrrak/sfc/20200424/hrrrak.t03z.wrfsfcf00.grib2 as ./20200424_hrrrak.t03z.wrfsfcf00.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrrak/sfc/20200424/hrrrak.t03z.wrfsfcf03.grib2 as ./20200424_hrrrak.t03z.wrfsfcf03.grib2\n",
      "\n",
      "Finished üç¶\n"
     ]
    }
   ],
   "source": [
    "download_HRRR(DATES, fxx, model='alaska', SOURCE='nomads', dryrun=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things about that above example...\n",
    "1. There is a **UserWarning** that says the `SOURCE` was changed to download from 'pando'. That is becuase the run DATES we requested are older than two days and are not available on NOMADS.\n",
    "2. A printed **WARNING** tells us the requested URL could not be found for a few of our requested files. That is becuase the HRRR-Alaska model only runs at 00z, 03z, 06z, 12z, 15z, 18z, and 21z. It does not run hourly like the HRRR model. When retrieveing Alaska files, you should set your date_range with a 3-hour interval (e.g. `DATES = pd.date_range(sDATE, eDATE, freq='3H')`).\n",
    "3. Remember that we ran this with `dryrun=True`, meaning we didn't actually download the files, but it told us what it would have downloaded and where it would have saved the file.\n",
    "\n",
    "> If you get `WARNING: Status code 404`, you might want to check that the file exists. One way to do that on Pando is to use the [interactive web download interface](home.chpc.utah.edu/~u0553130/Brian_Blaylock/cgi-bin/hrrr_download.cgi) and check if the file you are trying to download is available. You might want to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's request the F00-F19 forecasts from a single run from yesterday and do a \"dryrun\" to download files from NOMADS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåµ Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200625/conus/hrrr.t22z.wrfsfcf00.grib2 as ./20200625_hrrr.t22z.wrfsfcf00.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200625/conus/hrrr.t22z.wrfsfcf01.grib2 as ./20200625_hrrr.t22z.wrfsfcf01.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200625/conus/hrrr.t22z.wrfsfcf02.grib2 as ./20200625_hrrr.t22z.wrfsfcf02.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200625/conus/hrrr.t22z.wrfsfcf03.grib2 as ./20200625_hrrr.t22z.wrfsfcf03.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200625/conus/hrrr.t22z.wrfsfcf04.grib2 as ./20200625_hrrr.t22z.wrfsfcf04.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200625/conus/hrrr.t22z.wrfsfcf05.grib2 as ./20200625_hrrr.t22z.wrfsfcf05.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200625/conus/hrrr.t22z.wrfsfcf06.grib2 as ./20200625_hrrr.t22z.wrfsfcf06.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200625/conus/hrrr.t22z.wrfsfcf07.grib2 as ./20200625_hrrr.t22z.wrfsfcf07.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200625/conus/hrrr.t22z.wrfsfcf08.grib2 as ./20200625_hrrr.t22z.wrfsfcf08.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200625/conus/hrrr.t22z.wrfsfcf09.grib2 as ./20200625_hrrr.t22z.wrfsfcf09.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200625/conus/hrrr.t22z.wrfsfcf10.grib2 as ./20200625_hrrr.t22z.wrfsfcf10.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200625/conus/hrrr.t22z.wrfsfcf11.grib2 as ./20200625_hrrr.t22z.wrfsfcf11.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200625/conus/hrrr.t22z.wrfsfcf12.grib2 as ./20200625_hrrr.t22z.wrfsfcf12.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200625/conus/hrrr.t22z.wrfsfcf13.grib2 as ./20200625_hrrr.t22z.wrfsfcf13.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200625/conus/hrrr.t22z.wrfsfcf14.grib2 as ./20200625_hrrr.t22z.wrfsfcf14.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200625/conus/hrrr.t22z.wrfsfcf15.grib2 as ./20200625_hrrr.t22z.wrfsfcf15.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200625/conus/hrrr.t22z.wrfsfcf16.grib2 as ./20200625_hrrr.t22z.wrfsfcf16.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200625/conus/hrrr.t22z.wrfsfcf17.grib2 as ./20200625_hrrr.t22z.wrfsfcf17.grib2\n",
      "üåµ Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200625/conus/hrrr.t22z.wrfsfcf18.grib2 as ./20200625_hrrr.t22z.wrfsfcf18.grib2\n",
      "\n",
      "Finished üç¶\n"
     ]
    }
   ],
   "source": [
    "yesterday = datetime.utcnow() - timedelta(days=1)\n",
    "\n",
    "download_HRRR(yesterday, fxx=range(0,19), model='hrrr', SOURCE='nomads', dryrun=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### That's all folks!\n",
    "\n",
    "To keep things clean, you could copy the contents of the two functions `reporthook` and `download_HRRR` into a file named `my_functions.py` and import the `download_HRRR` function into another script with\n",
    "\n",
    "    from my_functions import download_HRRR\n",
    "\n",
    "And there you have it, a useful function to help you download a bunch of HRRR grib2 files from the University of Utah's HRRR archive on Pando and NOMADS.\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### Notes\n",
    "\n",
    "An alternative to the `urllib.request.urlretrieve(url, 'myfile.grib2')` method is to use the `requests` library and write the binary to a file.\n",
    "\n",
    "    import io\n",
    "    import requests\n",
    "    \n",
    "    url = 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200624/hrrr.t01z.wrfsfcf17.grib2'\n",
    "    r = requests.get(url)\n",
    "    with open('myNewFile.grib2', 'wb') as f:\n",
    "        f.write(io.BytesIO(r.content).getbuffer())\n",
    "\n",
    "Let's give this a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "\n",
    "url = 'https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200624/hrrr.t01z.wrfsfcf17.grib2'\n",
    "r = requests.get(url)\n",
    "with open('myNewFile.grib2', 'wb') as f:\n",
    "    f.write(io.BytesIO(r.content).getbuffer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at the file we downloaded...we see it is a valid GRIB2 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:7852240:d=2020062401:TMP:500 mb:17 hour fcst:\n",
      "19:11260176:d=2020062401:TMP:700 mb:17 hour fcst:\n",
      "24:14834021:d=2020062401:TMP:850 mb:17 hour fcst:\n",
      "28:17825413:d=2020062401:TMP:925 mb:17 hour fcst:\n",
      "32:20789480:d=2020062401:TMP:1000 mb:17 hour fcst:\n",
      "59:39358136:d=2020062401:TMP:surface:17 hour fcst:\n",
      "66:42415868:d=2020062401:TMP:2 m above ground:17 hour fcst:\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wgrib2 myNewFile.grib2 -match ':TMP:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
